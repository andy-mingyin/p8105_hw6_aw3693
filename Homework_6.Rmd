---
title: "Homework 6"
author: "Mingyin Wang"
date: "2024-12-02"
output: github_document
---

load packages
```{r, include=FALSE}
library(tidyverse)
library(modelr)
library(purrr)
library(p8105.datasets)
library(broom)
library(dplyr)
```

## Problem 1
load dataframe
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```
Bootstrap Function

```{r}
compute_bootstrap = function(data) {
  model = lm(tmax ~ tmin, data = data)
    r_squared = broom::glance(model)$r.squared
    coefs = broom::tidy(model)$estimate
  log_beta = log(coefs[1] * coefs[2])
  tibble(r_squared = r_squared, log_beta = log_beta)
}
```

 Generate 5000 bootstrap samples
```{r}
set.seed(1) 

bootstrap_results = weather_df |>
  modelr::bootstrap(n = 5000) |>
  mutate(
    estimates = map(strap, ~ compute_bootstrap(as.data.frame(.x)))  
  ) |>
  unnest(estimates)
```

Compute confidence interval
```{r}
bootstrap_summary = bootstrap_results |>
  summarise(
    r_squared_lower = quantile(r_squared, 0.025),
    r_squared_upper = quantile(r_squared, 0.975),
    log_beta_lower = quantile(log_beta, 0.025),
    log_beta_upper = quantile(log_beta, 0.975)
  )
bootstrap_summary
```

Plot the R-squared distribution
```{r}
ggplot(bootstrap_results, aes(x = r_squared)) +
  geom_histogram(binwidth = 0.005, fill = "blue", color = "black") +
  labs(title = "Bootstrap Distribution of r-squared", x = "r-squared", y = "Count")

```

Plot the log(B0*B1)
```{r}
ggplot(bootstrap_results, aes(x = log_beta)) +
  geom_histogram(binwidth = 0.01, fill = "blue", color = "black") +
  labs(title = "Bootstrap Distribution of log(beta_0 * beta_1)", x = "log(beta_0 * beta_1)", y = "Count")
```


The distribution is approximately symmetric, with values ranging from 1.95 to 2.10 and a peak around 2.02, indicating stable and consistent estimates across bootstrap samples. The narrow spread suggests robustness in the linear model used to predict tmax from tmin. A 95% confidence interval, derived from the 2.5% and 97.5% quantiles of the distribution, would provide a reliable range of plausible values for log(β0* β1). 


 
## Problem 2 

load data
```{r}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_data = read_csv(url)
head(homicide_data)
```

Create `city_state` variable and clean the data
```{r}
homicide_cleaned = homicide_data |>
  mutate(city_state = paste(city, state, sep = ", "),
         victim_age = as.numeric(victim_age)) |>
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
         victim_race %in% c("White", "Black"), 
         !is.na(victim_age),                  
         !is.na(disposition))        
head(homicide_cleaned)
```

Create a Binary Outcome Variable for Solved Cases
```{r}
 homicide_cleaned =  homicide_cleaned |>
  mutate(solved = if_else(str_detect(disposition, "Closed"), 1, 0))
```

Logistic Regression for Baltimore, MD
```{r}
baltimore_data = homicide_cleaned |>
  filter(city_state == "Baltimore, MD")

baltimore_model = glm(solved ~ victim_age + victim_sex + victim_race,
                       data = baltimore_data,
                       family = binomial)


baltimore_results = tidy(baltimore_model, conf.int = TRUE, exponentiate = TRUE)

baltimore_results
```
 Logistic Regression for All Cities
 
```{r}

nested_data = homicide_cleaned |>
  group_by(city_state) |>
  nest()

fit_model = function(df) {
  glm(solved ~ victim_age + victim_sex + victim_race, data = df, family = binomial) %>%
    tidy(conf.int = TRUE, exponentiate = TRUE) |>
    filter(term == "victim_sexMale")
}

results = nested_data |>
  mutate(model_results = map(data, fit_model)) |>
  unnest(model_results)

 
```

create the plot
```{r}

results |>
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    title = "Odds Ratios for Solving Homicides by City",
    x = "City",
    y = "Odds Ratio (Male vs Female Victims)"
  ) +
  theme_minimal()
```

An odds ratio of 1 indicates no difference between genders, while values greater than 1 suggest a higher likelihood of solving homicides for male victims, and values less than 1 indicate a higher likelihood for female victims. The horizontal lines represent 95% confidence intervals, where narrower intervals indicate greater precision, and wider intervals suggest more uncertainty. Most cities cluster around an odds ratio of 1, indicating little to no gender-based difference in solving homicides, though some cities show significant deviations with varying confidence intervals, possibly due to small sample sizes or other factors.

## Problem 3

load data
```{r}
birthweight_data = read_csv("data/birthweight.csv")
head(birthweight_data)
```

```{r}

birthweight_data_cleaned = birthweight_data |>
  mutate(
    babysex = factor(babysex, levels = c(1, 2), labels = c("Male", "Female")),
    malform = factor(malform, levels = c(0, 1), labels = c("Absent", "Present")),
    mrace = factor(mrace),
    frace = factor(frace)
  ) |>
  drop_na() 
```

linear model
```{r}
proposed_model = lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + wtgain, 
                     data = birthweight_data_cleaned)
summary(proposed_model)
```
The variables included in the model to predict birth weight are chosen for their biological relevance. Baby's sex, head circumference, and length at birth are direct indicators of size and growth. Maternal weight at delivery and weight gain during pregnancy reflect maternal health and nutrition, which influence fetal growth. Gestational age captures the duration of pregnancy, a key determinant of birth weight.


use add_predictions and add_residuals in making this plot
```{r}
data_with_residuals = birthweight_data_cleaned |>
  add_predictions(proposed_model) |>
  add_residuals(proposed_model)

data_with_residuals |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    title = "Residuals vs Fitted Values",
    x = "Fitted Values (Predicted Birth Weight)",
    y = "Residuals"
  ) +
  theme_minimal()
```
two other models
One using length at birth and gestational age as predictors (main effects only)
One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
```{r}
proposed_formula = bwt ~ gaweeks + blength + bhead + wtgain
model1_formula = bwt ~ blength + gaweeks
model2_formula = bwt ~ bhead * blength * babysex
```

cross validation
```{r}
set.seed(123)
cv_data = crossv_mc(birthweight_data_cleaned, 10) 


```

Define RMSE computation function
```{r}

compute_rmse = function(train_data, test_data, formula) {
  model = lm(formula, data = train_data)
  predictions = predict(model, newdata = test_data)
  sqrt(mean((test_data$bwt - predictions)^2))
}
```

Compute RMSE for each model
```{r}
cv_results = cv_data |>
  mutate(
    proposed_rmse = map2_dbl(train, test, ~ compute_rmse(as.data.frame(.x), as.data.frame(.y), proposed_formula)),
    model1_rmse = map2_dbl(train, test, ~ compute_rmse(as.data.frame(.x), as.data.frame(.y), model1_formula)),
    model2_rmse = map2_dbl(train, test, ~ compute_rmse(as.data.frame(.x), as.data.frame(.y), model2_formula))
  )
```


```{r}
cv_results_summary = cv_results |>
  summarise(
    proposed_avg_rmse = mean(proposed_rmse),
    model1_avg_rmse = mean(model1_rmse),
    model2_avg_rmse = mean(model2_rmse)
  )
cv_results_summary
```
Proposed Model: It performs well due to inclusion of relevant predictors.
Model 1: It likely has higher RMSE since it uses fewer predictors.
Model 2: It maight overfit due to the inclusion of multiple interactions.

Then, we should use the proposed model, since it has the lowest RMSE, 



